{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WkOA4mcN7Hj"
   },
   "source": [
    "# An Even Easier Introduction to CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuOcUi0fvogW"
   },
   "source": [
    "This notebook is a Colab-friendly companion to NVIDIA’s free DLI course:\n",
    "\n",
    "- **An Even Easier Introduction to CUDA**: https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+T-AC-01+V1\n",
    "\n",
    "It follows the same learning path as Mark Harris’s blog post (the course companion reading) and guides you through progressively more parallel CUDA kernels for adding two vectors of **1,048,576** floats.\n",
    "\n",
    "### What you’ll do here\n",
    "By the end of the notebook, you will be able to:\n",
    "\n",
    "- Compile and run **CUDA C++** code with `nvcc`\n",
    "- Launch CUDA kernels with different execution configurations (threads and blocks)\n",
    "- Use **Unified Memory** and understand why data migration matters\n",
    "- Measure kernel runtime using **CUDA events** (and optionally collect a timeline with Nsight Systems, if available)\n",
    "\n",
    "### Before you run\n",
    "1. In Colab: **Runtime → Change runtime type → GPU**\n",
    "2. Check the GPU model with `nvidia-smi`.\n",
    "3. This notebook compiles with `-arch=sm_75` (T4). If Colab gives you a different GPU, change the `-arch` flag in the compile cells:\n",
    "   - P100: `sm_60` • V100: `sm_70` • T4: `sm_75` • A100: `sm_80` • L4: `sm_89`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xfb0QUHnIjq_",
    "outputId": "5402c55a-f449-4cf4-c97d-914de89aa09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  3 23:13:25 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   31C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "nsys not on PATH\n",
      "/opt/nvidia/nsight-compute/2024.2.1/host/target-linux-x64/nsys\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!which nsys || echo \"nsys not on PATH\"\n",
    "!find / -type f -name nsys 2>/dev/null | head -n 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hf5TF8PmJC3V",
    "outputId": "f69b293b-971a-4618-d892-60310538f026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-03 23:14:39--  https://developer.nvidia.com/downloads/assets/tools/secure/nsight-systems/2025_6/NsightSystems-linux-cli-public-2025.6.1.190-3689520.deb\n",
      "Resolving developer.nvidia.com (developer.nvidia.com)... 23.59.88.80, 23.59.88.69, 23.59.88.67, ...\n",
      "Connecting to developer.nvidia.com (developer.nvidia.com)|23.59.88.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://developer.download.nvidia.com/assets/tools/secure/nsight-systems/2025_6/NsightSystems-linux-cli-public-2025.6.1.190-3689520.deb?__token__=exp=1767483879~hmac=cf2ebad1ead6d659eca139b9e61dfba644bc784b61646f05f9ea3fd54774ad29 [following]\n",
      "--2026-01-03 23:14:39--  https://developer.download.nvidia.com/assets/tools/secure/nsight-systems/2025_6/NsightSystems-linux-cli-public-2025.6.1.190-3689520.deb?__token__=exp=1767483879~hmac=cf2ebad1ead6d659eca139b9e61dfba644bc784b61646f05f9ea3fd54774ad29\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.59.88.9, 23.59.88.13, 23.59.88.3, ...\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.59.88.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 207129842 (198M) [application/x-deb]\n",
      "Saving to: ‘nsys_cli.deb’\n",
      "\n",
      "nsys_cli.deb        100%[===================>] 197.53M  39.7MB/s    in 4.7s    \n",
      "\n",
      "2026-01-03 23:14:44 (41.7 MB/s) - ‘nsys_cli.deb’ saved [207129842/207129842]\n",
      "\n",
      "Selecting previously unselected package nsight-systems-cli-2025.6.1.\n",
      "(Reading database ... 121689 files and directories currently installed.)\n",
      "Preparing to unpack nsys_cli.deb ...\n",
      "Unpacking nsight-systems-cli-2025.6.1 (2025.6.1.190-256136895201v0) ...\n",
      "Setting up nsight-systems-cli-2025.6.1 (2025.6.1.190-256136895201v0) ...\n",
      "update-alternatives: using /opt/nvidia/nsight-systems-cli/2025.6.1/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
      "NVIDIA Nsight Systems version 2025.6.1.190-256136895201v0\n"
     ]
    }
   ],
   "source": [
    "# Download (CLI-only .deb)\n",
    "!wget -O nsys_cli.deb \"https://developer.nvidia.com/downloads/assets/tools/secure/nsight-systems/2025_6/NsightSystems-linux-cli-public-2025.6.1.190-3689520.deb\"\n",
    "\n",
    "# Install\n",
    "!sudo dpkg -i nsys_cli.deb || sudo apt-get -f install -y\n",
    "\n",
    "# Verify\n",
    "!nsys --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1C6GK_MO5er"
   },
   "source": [
    "<img src=\"https://developer.download.nvidia.com/training/courses/T-AC-01-V1/CUDA_Cube_1K.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcmbR8lZPLRv"
   },
   "source": [
    "This post is a super simple introduction to CUDA, the popular parallel computing platform and programming model from NVIDIA. I wrote a previous [“Easy Introduction”](https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/) to CUDA in 2013 that has been very popular over the years. But CUDA programming has gotten easier, and GPUs have gotten much faster, so it’s time for an updated (and even easier) introduction.\n",
    "\n",
    "CUDA C++ is just one of the ways you can create massively parallel applications with CUDA. It lets you use the powerful C++ programming language to develop high performance algorithms accelerated by thousands of parallel threads running on GPUs. Many developers have accelerated their computation- and bandwidth-hungry applications this way, including the libraries and frameworks that underpin the ongoing revolution in artificial intelligence known as [Deep Learning](https://developer.nvidia.com/deep-learning).\n",
    "\n",
    "So, you’ve heard about CUDA and you are interested in learning how to use it in your own applications. If you are a C or C++ programmer, this blog post should give you a good start. To follow along, you’ll need a computer with an CUDA-capable GPU (Windows, Mac, or Linux, and any NVIDIA GPU should do), or a cloud instance with GPUs (AWS, Azure, IBM SoftLayer, and other cloud service providers have them). You’ll also need the free [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) installed.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDQ9ycz0Qfyf"
   },
   "source": [
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_ai_cube-625x625.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH9Rfms_QtXF"
   },
   "source": [
    "## Starting Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ab718",
   "metadata": {},
   "source": [
    "### The three CUDA versions you’ll run\n",
    "You’ll write and run three CUDA programmes (each saved to its own `.cu` file):\n",
    "\n",
    "1. **`add.cu` — 1 thread**  \n",
    "   A first CUDA kernel launch (`<<<1,1>>>`) to show the basic mechanics. Correct, but not parallel.\n",
    "\n",
    "2. **`add_block.cu` — 256 threads (1 block)**  \n",
    "   Uses `threadIdx.x` and a *block-stride* loop so the work is shared across threads in a single block.\n",
    "\n",
    "3. **`add_grid.cu` — many blocks × 256 threads (grid-stride loop)**  \n",
    "   Scales across the whole GPU using `blockIdx.x`, `gridDim.x`, and a *grid-stride* loop. This version also demonstrates **Unified Memory prefetching** and prints an **average kernel time** from multiple iterations.\n",
    "\n",
    "You’ll keep the algorithm the same (vector add) so you can focus on how launch configuration and memory behaviour affect performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5-iUihBQvQt"
   },
   "source": [
    "We’ll start with a simple C++ program that adds the elements of two arrays with a million elements each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nc-gBqLDQ7AC",
    "outputId": "3896f934-705b-4ba0-9be9-ac4df3c6b116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile add.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "\n",
    "// function to add the elements of two arrays\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  for (int i = 0; i < n; i++)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  int N = 1<<20; // 1M elements\n",
    "\n",
    "  float *x = new float[N];\n",
    "  float *y = new float[N];\n",
    "\n",
    "  // initialize x and y arrays on the host\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // Run kernel on 1M elements on the CPU\n",
    "  add(N, x, y);\n",
    "\n",
    "  // Check for errors (all values should be 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  // Free memory\n",
    "  delete [] x;\n",
    "  delete [] y;\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw6DsX4uRHMg"
   },
   "source": [
    "Executing the above cell will save its contents to the file add.cpp.\n",
    "\n",
    "The following cell will compile and run this C++ program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNpH54M_RbAU",
    "outputId": "f75ad7ba-9715-4fd7-9556-6f1fd4432940"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "g++ add.cpp -o add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6V2tGPYRi3l"
   },
   "source": [
    "Then run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmA4ACe5RuiU",
    "outputId": "bd6bfc1d-4c95-48a9-cc85-129238f42966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "./add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IAWYlniR153"
   },
   "source": [
    "As expected, it prints that there was no error in the summation and then exits. Now I want to get this computation running (in parallel) on the many cores of a GPU. It’s actually pretty easy to take the first steps.\n",
    "\n",
    "First, I just have to turn our `add` function into a function that the GPU can run, called a *kernel* in CUDA. To do this, all I have to do is add the specifier `__global__` to the function, which tells the CUDA C++ compiler that this is a function that runs on the GPU and can be called from CPU code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heY-lpzjSHfB"
   },
   "source": [
    "```cpp\n",
    "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  for (int i = 0; i < n; i++)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kozMbHdpSKNu"
   },
   "source": [
    "These `__global__` functions are known as *kernels*, and code that runs on the GPU is often called *device code*, while code that runs on the CPU is *host code*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhnBGGU-SWiN"
   },
   "source": [
    "## Memory Allocation in CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvIDRBk2SbqA"
   },
   "source": [
    "To compute on the GPU, I need to allocate memory accessible by the GPU. [Unified Memory](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/) in CUDA makes this easy by providing a single memory space accessible by all GPUs and CPUs in your system. To allocate data in unified memory, call `cudaMallocManaged()`, which returns a pointer that you can access from host (CPU) code or device (GPU) code. To free the data, just pass the pointer to `cudaFree()`.\n",
    "\n",
    "I just need to replace the calls to `new` in the code above with calls to `cudaMallocManaged()`, and replace calls to `delete []` with calls to `cudaFree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxCut_urS46H"
   },
   "source": [
    "```cpp\n",
    "  // Allocate Unified Memory -- accessible from CPU or GPU\n",
    "  float *x, *y;\n",
    "  cudaMallocManaged(&x, N*sizeof(float));\n",
    "  cudaMallocManaged(&y, N*sizeof(float));\n",
    "\n",
    "  ...\n",
    "\n",
    "  // Free memory\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oEf2B-1S-1V"
   },
   "source": [
    "Finally, I need to *launch* the `add()` kernel, which invokes it on the GPU. CUDA kernel launches are specified using the triple angle bracket syntax `<<< >>>`. I just have to add it to the call to `add` before the parameter list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqTJlvWLS7iW"
   },
   "source": [
    "```cpp\n",
    "add<<<1, 1>>>(N, x, y);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGf0ZiTOTTHU"
   },
   "source": [
    "Easy! I’ll get into the details of what goes inside the angle brackets soon; for now all you need to know is that this line launches one GPU thread to run `add()`.\n",
    "\n",
    "Just one more thing: I need the CPU to wait until the kernel is done before it accesses the results (because CUDA kernel launches don’t block the calling CPU thread). To do this I just call `cudaDeviceSynchronize()` before doing the final error checking on the CPU.\n",
    "\n",
    "Here’s the complete code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8bYDM7kYT7S",
    "outputId": "6c390344-f0af-4ff0-d369-2d9afc2536f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting add.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add.cu\n",
    "#include <iostream>\n",
    "#include <cmath>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__\n",
    "void add(int n, const float *x, float *y)\n",
    "{\n",
    "  // single-thread version (as in your example)\n",
    "  for (int i = 0; i < n; i++)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  const int N = 1 << 20; // 1M\n",
    "\n",
    "  float *x = nullptr, *y = nullptr;\n",
    "  cudaMallocManaged(&x, N * sizeof(float));\n",
    "  cudaMallocManaged(&y, N * sizeof(float));\n",
    "\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // --- simple kernel timing using CUDA events ---\n",
    "  cudaEvent_t start, stop;\n",
    "  cudaEventCreate(&start);\n",
    "  cudaEventCreate(&stop);\n",
    "\n",
    "  cudaEventRecord(start);\n",
    "  add<<<1, 1>>>(N, x, y);\n",
    "  cudaEventRecord(stop);\n",
    "\n",
    "  cudaEventSynchronize(stop);\n",
    "\n",
    "  float ms = 0.0f;\n",
    "  cudaEventElapsedTime(&ms, start, stop);\n",
    "  std::cout << \"Kernel time (ms): \" << ms << std::endl;\n",
    "\n",
    "  cudaDeviceSynchronize(); // ensure results ready\n",
    "\n",
    "  // check result\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  cudaEventDestroy(start);\n",
    "  cudaEventDestroy(stop);\n",
    "\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjLGGp0oYeEc",
    "outputId": "44ecd2d9-dc1c-4486-b47a-9b38499a5072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel time (ms): 107.89\n",
      "Max error: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "nvcc -O3 add.cu -o add -arch=sm_75\n",
    "./add\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ATssEzEYqGx"
   },
   "source": [
    "This is only a first step, because as written, this kernel is only correct for a single thread, since every thread that runs it will perform the add on the whole array. Moreover, there is a [race condition](https://en.wikipedia.org/wiki/Race_condition) since multiple parallel threads would both read and write the same locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kKpDoZ-YzJ8"
   },
   "source": [
    "## Profile it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-BC-CWVZglt"
   },
   "source": [
    "For quick feedback, this notebook measures runtime **inside the programme** using **CUDA events**.  \n",
    "\n",
    "- `add.cu` and `add_block.cu` print a single **Kernel time (ms)**  \n",
    "- `add_grid.cu` runs the kernel many times and prints an **Avg kernel time (ms)**\n",
    "\n",
    "For a deeper look (timelines, memory operations, concurrency), you can use profiling tools such as **Nsight Systems** (`nsys`) or **Nsight Compute** (`ncu`) when available in your environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Dn4ZV-Z_UJ"
   },
   "source": [
    "Kernel timings will vary depending on which GPU Colab assigns to you.  \n",
    "Run the next cell to see your current GPU model (for example, *Tesla T4*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrYmwVZfaPqz",
    "outputId": "f29a2675-5e9c-4822-f840-ab7806d9631a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  3 22:15:49 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MWYteAVadCs"
   },
   "source": [
    "Let's make it faster with parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaiMC73Falvb"
   },
   "source": [
    "## Picking up the Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFuBr_2apuJ"
   },
   "source": [
    "Now that you’ve run a kernel with one thread that does some computation, how do you make it parallel? The key is in CUDA’s `<<<1, 1>>>` syntax. This is called the execution configuration, and it tells the CUDA runtime how many parallel threads to use for the launch on the GPU. There are two parameters here, but let’s start by changing the second one: the number of threads in a thread block. CUDA GPUs run kernels using blocks of threads that are a multiple of 32 in size, so 256 threads is a reasonable size to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2Pmyj0KavgB"
   },
   "source": [
    "```cpp\n",
    "add<<<1, 256>>>(N, x, y);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAYpH9Ctay5G"
   },
   "source": [
    "If I run the code with only this change, it will do the computation once per thread, rather than spreading the computation across the parallel threads. To do it properly, I need to modify the kernel. CUDA C++ provides keywords that let kernels get the indices of the running threads. Specifically, `threadIdx.x` contains the index of the current thread within its block, and `blockDim.x` contains the number of threads in the block. I’ll just modify the loop to stride through the array with parallel threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSiqhFK_a6N3"
   },
   "source": [
    "```cpp\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  int index = threadIdx.x;\n",
    "  int stride = blockDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7mYcBzOa9zR"
   },
   "source": [
    "The `add` function hasn’t changed that much. In fact, setting `index = 0` and `stride = 1` makes it semantically identical to the single-thread version.\n",
    "\n",
    "Now we’ll save the file as **`add_block.cu`**, compile it, and run it.  \n",
    "When it runs, it will print **Kernel time (ms)** and **Max error** so you can compare performance and correctness with the previous version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goCKY9QNbPZ-",
    "outputId": "c4fc7160-7760-4c2e-cfbe-30dabf628945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting add_block.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_block.cu\n",
    "#include <iostream>\n",
    "#include <cmath>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__\n",
    "void add(int n, const float *x, float *y)\n",
    "{\n",
    "  int index  = threadIdx.x;\n",
    "  int stride = blockDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  const int N = 1 << 20;\n",
    "\n",
    "  float *x = nullptr, *y = nullptr;\n",
    "  cudaMallocManaged(&x, N * sizeof(float));\n",
    "  cudaMallocManaged(&y, N * sizeof(float));\n",
    "\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // --- simple kernel timing using CUDA events ---\n",
    "  cudaEvent_t start, stop;\n",
    "  cudaEventCreate(&start);\n",
    "  cudaEventCreate(&stop);\n",
    "\n",
    "  cudaEventRecord(start);\n",
    "  add<<<1, 256>>>(N, x, y);\n",
    "  cudaEventRecord(stop);\n",
    "\n",
    "  cudaEventSynchronize(stop);\n",
    "\n",
    "  float ms = 0.0f;\n",
    "  cudaEventElapsedTime(&ms, start, stop);\n",
    "  std::cout << \"Kernel time (ms): \" << ms << std::endl;\n",
    "\n",
    "  cudaDeviceSynchronize(); // ensure results ready\n",
    "\n",
    "  // check result\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++)\n",
    "    maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  cudaEventDestroy(start);\n",
    "  cudaEventDestroy(stop);\n",
    "\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9cmfbcVbYgD",
    "outputId": "dcd498ad-d73f-4cf9-bc13-88a294ccd282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel time (ms): 3.60054\n",
      "Max error: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "nvcc -O3 add_block.cu -o add_block -arch=sm_75\n",
    "./add_block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo5KaV3Nba7g"
   },
   "source": [
    "That’s a big speed-up — compare the printed **Kernel time (ms)** from this run with the previous single-thread launch.  \n",
    "That improvement is expected because we increased parallelism from **1 thread** to **256 threads**. Next, we’ll scale out to *many* blocks so the whole GPU can participate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtgQWOyMcPfn"
   },
   "source": [
    "## Out of the Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAoFGwmbcRbN"
   },
   "source": [
    "CUDA GPUs have many parallel processors grouped into Streaming Multiprocessors, or SMs. Each SM can run multiple concurrent thread blocks. As an example, a Tesla P100 GPU based on the [Pascal GPU Architecture](https://developer.nvidia.com/blog/inside-pascal/) has 56 SMs, each capable of supporting up to 2048 active threads. To take full advantage of all these threads, I should launch the kernel with multiple thread blocks.\n",
    "\n",
    "By now you may have guessed that the first parameter of the execution configuration specifies the number of thread blocks. Together, the blocks of parallel threads make up what is known as the *grid*. Since I have `N` elements to process, and 256 threads per block, I just need to calculate the number of blocks to get at least `N` threads. I simply divide `N` by the block size (being careful to round up in case `N` is not a multiple of `blockSize`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnI2II2ockgC"
   },
   "source": [
    "```cpp\n",
    "int blockSize = 256;\n",
    "int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayq2MJZLctY0"
   },
   "source": [
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZduP7RWc3Je"
   },
   "source": [
    "I also need to update the kernel code to take into account the entire grid of thread blocks. CUDA provides `gridDim.x`, which contains the number of blocks in the grid, and `blockIdx.x`, which contains the index of the current thread block in the grid. Figure 1 illustrates the the approach to indexing into an array (one-dimensional) in CUDA using `blockDim.x`, `gridDim.x`, and `threadIdx.x`. The idea is that each thread gets its index by computing the offset to the beginning of its block (the block index times the block size: `blockIdx.x * blockDim.x`) and adding the thread’s index within the block (`threadIdx.x`). The code `blockIdx.x * blockDim.x + threadIdx.x` is idiomatic CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cI2WLEAeG5y"
   },
   "source": [
    "```cpp\n",
    "__global__\n",
    "void add(int n, float *x, float *y)\n",
    "{\n",
    "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int stride = blockDim.x * gridDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83hC-rCLdPHC"
   },
   "source": [
    "The updated kernel sets:\n",
    "\n",
    "- `index = blockIdx.x * blockDim.x + threadIdx.x` (a unique starting index per thread across the whole grid)\n",
    "- `stride = blockDim.x * gridDim.x` (the total number of threads in the grid)\n",
    "\n",
    "This pattern is called a **grid-stride loop** and is a standard way to write kernels that scale to any `N`.\n",
    "\n",
    "Now save the file as **`add_grid.cu`**, compile it, and run it.  \n",
    "This version also demonstrates **Unified Memory prefetching** and prints an **Avg kernel time (ms)** over multiple iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hklSSgPWKGVm",
    "outputId": "14480891-873a-4f64-ed26-6ae76c4f3e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_grid.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_grid.cu\n",
    "#include <iostream>\n",
    "#include <cmath>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define CUDA_CHECK(call) do {                                           \\\n",
    "  cudaError_t err = (call);                                             \\\n",
    "  if (err != cudaSuccess) {                                             \\\n",
    "    std::cerr << \"CUDA error at \" << __FILE__ << \":\" << __LINE__        \\\n",
    "              << \" : \" << cudaGetErrorString(err) << std::endl;         \\\n",
    "    return 1;                                                           \\\n",
    "  }                                                                     \\\n",
    "} while (0)\n",
    "\n",
    "__global__\n",
    "void add(int n, const float *x, float *y)\n",
    "{\n",
    "  int index  = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int stride = blockDim.x * gridDim.x;\n",
    "  for (int i = index; i < n; i += stride)\n",
    "    y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  const int N = 1 << 20; // 1M\n",
    "  float *x = nullptr, *y = nullptr;\n",
    "\n",
    "  CUDA_CHECK(cudaMallocManaged(&x, N * sizeof(float)));\n",
    "  CUDA_CHECK(cudaMallocManaged(&y, N * sizeof(float)));\n",
    "\n",
    "  // init on CPU\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    x[i] = 1.0f;\n",
    "    y[i] = 2.0f;\n",
    "  }\n",
    "\n",
    "  // launch config\n",
    "  const int blockSize = 256;\n",
    "  const int numBlocks = (N + blockSize - 1) / blockSize;\n",
    "\n",
    "  // Prefetch Unified Memory to the active GPU\n",
    "  int dev = 0;\n",
    "  CUDA_CHECK(cudaGetDevice(&dev));\n",
    "  CUDA_CHECK(cudaMemPrefetchAsync(x, N * sizeof(float), dev));\n",
    "  CUDA_CHECK(cudaMemPrefetchAsync(y, N * sizeof(float), dev));\n",
    "  CUDA_CHECK(cudaDeviceSynchronize());\n",
    "\n",
    "  // Warm-up (also helps JIT / first-touch effects)\n",
    "  add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "  CUDA_CHECK(cudaGetLastError());\n",
    "  CUDA_CHECK(cudaDeviceSynchronize());\n",
    "\n",
    "  // Timed loop using CUDA events\n",
    "  cudaEvent_t start, stop;\n",
    "  CUDA_CHECK(cudaEventCreate(&start));\n",
    "  CUDA_CHECK(cudaEventCreate(&stop));\n",
    "\n",
    "  const int iters = 100;\n",
    "\n",
    "  CUDA_CHECK(cudaEventRecord(start));\n",
    "  for (int r = 0; r < iters; r++) {\n",
    "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "  }\n",
    "  CUDA_CHECK(cudaGetLastError());\n",
    "  CUDA_CHECK(cudaEventRecord(stop));\n",
    "  CUDA_CHECK(cudaEventSynchronize(stop));\n",
    "\n",
    "  float ms = 0.0f;\n",
    "  CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n",
    "\n",
    "  std::cout << \"Avg kernel time (ms): \" << (ms / iters) << std::endl;\n",
    "\n",
    "  // Validate (expect y ~= 3.0f)\n",
    "  float maxError = 0.0f;\n",
    "  for (int i = 0; i < N; i++) {\n",
    "    maxError = fmax(maxError, fabs(y[i] - 3.0f));\n",
    "  }\n",
    "  std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "  CUDA_CHECK(cudaEventDestroy(start));\n",
    "  CUDA_CHECK(cudaEventDestroy(stop));\n",
    "  CUDA_CHECK(cudaFree(x));\n",
    "  CUDA_CHECK(cudaFree(y));\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XT8lJifJKJFC",
    "outputId": "d7295167-e090-4a40-8d53-138774576de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg kernel time (ms): 0.0535261\n",
      "Max error: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "nvcc -O3 add_grid.cu -o add_grid -arch=sm_75\n",
    "./add_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c05882",
   "metadata": {},
   "source": [
    "### Optional: capture a GPU timeline with Nsight Systems\n",
    "If `nsys` is available in your runtime, you can record a short CUDA trace for the **grid** version.  \n",
    "This produces an `.nsys-rep` report file which you can download and open in the Nsight Systems GUI on your machine.\n",
    "\n",
    "If the command is not found, you can skip this step — the printed CUDA-event timings above are enough for this notebook’s comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Har9_ltmIpnI",
    "outputId": "a16759ef-98c3-480d-cd79-a79a94f93c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Max error: 1\n",
      "Generating '/tmp/nsys-report-b04d.qdstrm'\n",
      "[1/1] [========================100%] add_profile.nsys-rep\n",
      "Generated:\n",
      "\t/content/add_profile.nsys-rep\n",
      "-rw-rw-r-- 1 root root 75K Jan  3 23:15 add_profile.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -t cuda -o add_profile ./add_grid\n",
    "!ls -lh add_profile.nsys-rep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7Tz-xo3d1oX"
   },
   "source": [
    "That’s another big speed-up from running **multiple blocks** across the GPU.  \n",
    "Compare the **Avg kernel time (ms)** printed by `add_grid.cu` with the earlier versions.\n",
    "\n",
    "Your exact speed-ups will vary depending on the GPU Colab assigns. If your final speed-up is smaller than expected, the exercises below include ideas to investigate (especially memory behaviour and launch configuration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja5CiQZpicHC"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEijwk25id3t"
   },
   "source": [
    "To keep you going, here are a few things to try on your own.\n",
    "\n",
    "1. Browse the [CUDA Toolkit documentation](https://docs.nvidia.com/cuda/index.html). If you haven’t installed CUDA yet, check out the [Quick Start Guide](https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html) and the installation guides. Then browse the [Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) and the [Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html). There are also tuning guides for various architectures.\n",
    "2. Experiment with `printf()` inside the kernel. Try printing out the values of `threadIdx.x` and `blockIdx.x` for some or all of the threads. Do they print in sequential order? Why or why not?\n",
    "3. Print the value of `threadIdx.y` or `threadIdx.z` (or `blockIdx.y`) in the kernel. (Likewise for `blockDim` and `gridDim`). Why do these exist? How do you get them to take on values other than 0 (1 for the dims)?\n",
    "4. If you have access to a [Pascal-based GPU](https://developer.nvidia.com/blog/inside-pascal/), try running `add_grid.cu` on it. Is performance better or worse than the K80 results? Why? (Hint: read about [Pascal’s Page Migration Engine and the CUDA 8 Unified Memory API](https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/).) For a detailed answer to this question, see the post [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpWVnIPujp0K"
   },
   "source": [
    "## Where to From Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTyQePjlkRJ3"
   },
   "source": [
    "If you enjoyed this notebook and want to learn more, the [NVIDIA DLI](https://nvidia.com/dli) offers several in depth CUDA Programming courses.\n",
    "\n",
    "For those of you just starting out, please consider [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) which provides dedicated GPU resources, a more sophisticated programming environment, use of the [NVIDIA Nsight Systems™](https://developer.nvidia.com/nsight-systems) visual profiler, dozens of interactive exercises, detailed presentations, over 8 hours of material, and the ability to earn a DLI Certificate of Competency.\n",
    "\n",
    "Similarly, for Python programmers, please consider [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about).\n",
    "\n",
    "For more intermediate and advance CUDA programming materials, please check out the _Accelerated Computing_ section of the NVIDIA DLI [self-paced catalog](https://www.nvidia.com/en-us/training/online/)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "An Even Easier Introduction to CUDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
